{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Image_Classifier_GitHub.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/singminhong/flower-species-classifier/blob/master/Image_Classifier_GitHub.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "xgzjXhdRk3_R",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In this project an image classifier is trained to recognize different species of flowers using the Oxford dataset of 102 flower categories.\n",
        "\n",
        "The project is broken down into:\n",
        "\n",
        "* Create a test dataset\n",
        "\n",
        "* Load and preprocess the image dataset\n",
        "\n",
        "* Train the image classifier on the dataset\n",
        "\n",
        "* Use the trained classifier to predict image content\n",
        "\n",
        "Source files https://github.com/udacity/pytorch_challenge"
      ]
    },
    {
      "metadata": {
        "id": "DhTX9iWKkw30",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline\n",
        "\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import OrderedDict\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "from torchvision import datasets, transforms, models\n",
        "\n",
        "import scipy.io as sio\n",
        "from pathlib import Path\n",
        "from shutil import copy\n",
        "import time\n",
        "#import copy\n",
        "import shutil\n",
        "import os\n",
        "import random"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "onnCJScJn_lK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Data Downloading and Creation of a Test Dataset\n",
        "Create a test folder (with its labels) for the Udacity PyTorch Challenge flower dataset that contains the images in the original Oxford dataset that are not in the Udacity dataset.\n",
        "\n",
        "The dataset will then have three parts: training, validation and test."
      ]
    },
    {
      "metadata": {
        "id": "zi2FKh_Lkz-Z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Download the original dataset of 102 different categories of flowers common to the UK \n",
        "# from the Visual Geometry Group, Department of Engineering Science, University of Oxford \n",
        "# http://www.robots.ox.ac.uk/~vgg/data/flowers/\n",
        "!wget  -O ./assets/102flowers.tgz \"http://www.robots.ox.ac.uk/~vgg/data/flowers/102/102flowers.tgz\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "j1PYTBsxoGBq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Untar the Oxford dataset\n",
        "!tar xzf ./assets/102flowers.tgz -C ./assets/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_Z6JgXNuoJdz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## Download the Oxford labels\n",
        "!wget  -O ./assets/imagelabels.mat \"http://www.robots.ox.ac.uk/~vgg/data/flowers/102/imagelabels.mat\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "W07BV4MzoLP6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Download the Udacity PyTorch Challenge flower dataset \n",
        "!wget  -O ./assets/flower_data.zip  \"https://s3.amazonaws.com/content.udacity-data.com/courses/nd188/flower_data.zip\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "o_CFCxOmoOyg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Unzip the Udacity dataset\n",
        "!unzip ./assets/flower_data.zip -d ./assets/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OlSlat0roTPH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "root_dir = Path('./assets')\n",
        "original_dir = root_dir/'jpg'\n",
        "labels_file = root_dir/'imagelabels.mat'\n",
        "udacity_dir = root_dir/'flower_data'\n",
        "udacity_train_dir = udacity_dir/'train'\n",
        "udacity_valid_dir = udacity_dir/'valid'\n",
        "udacity_test_dir = udacity_dir/'test'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_LS097jFoWDK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Script prepared by a student in the Udacity PyTorch Scholarship Challenge, apologies that \n",
        "# I didn't note their name\n",
        "labels=sio.loadmat(labels_file)['labels'][0]\n",
        "(_, _, original_images) = next(os.walk(original_dir))\n",
        "original_images = sorted(original_images)\n",
        "image_to_label = {name: labels[i] for i, name in enumerate(original_images)}\n",
        "udacity_images = []\n",
        "for root, dirs, files in os.walk(udacity_dir): udacity_images.extend(files)\n",
        "diff = set(original_images) - set(udacity_images)\n",
        "udacity_test_dir.mkdir(parents=True, exist_ok=True)\n",
        "for file in diff:\n",
        "    dest_dir = udacity_test_dir/str(image_to_label[file])\n",
        "    dest_dir.mkdir(parents=True, exist_ok=True)\n",
        "    copy(original_dir/file, dest_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ngkeXVXJoYIM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Data Loading and Exploration\n",
        "For the training, transformations such as random scaling, cropping, and flipping are applied. This helps the network generalize leading to better performance. Input data is resized to 224x224 pixels as required by the networks.\n",
        "\n",
        "The validation set is used to measure the model's performance on data it hasn't seen yet. For this scaling or rotation transformations are not used, but the images are resized then cropped to the appropriate size.\n",
        "\n",
        "The test set was created by a student in the challenge by comparing the data selected by Udacity to the original data.\n",
        "\n",
        "The pre-trained networks available from torchvision were trained on the ImageNet dataset where each colour channel was normalized separately. The image data here needs to be normalized using the same means and standard deviations: [0.485, 0.456, 0.406] [0.229, 0.224, 0.225], as those calculated from the ImageNet images. These values will shift each colour channel to be centered at 0 and range from -1 to 1."
      ]
    },
    {
      "metadata": {
        "id": "cjLC2Eg3ocoL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Load and preprocess the image dataset\n",
        "data_dir = './assets/flower_data'\n",
        "train_dir = data_dir + '/train'\n",
        "valid_dir = data_dir + '/valid'\n",
        "test_dir = data_dir + '/test'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "k6itNvAyoeoE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "image = Image.open(train_dir+'/54/image_05459.jpg')\n",
        "plt.imshow(image);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ItGi_7aPoiCL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Exploratory Data Analysis\n",
        "def eda_counts():\n",
        "    dirs = [train_dir,valid_dir,test_dir]\n",
        "    for directory in dirs:\n",
        "        counts = []\n",
        "        total_images = 0\n",
        "        min_count = 100\n",
        "        max_count = 0\n",
        "        print ('Dataset: ',directory)\n",
        "        folders = ([name for name in os.listdir(directory)])\n",
        "        for folder in folders:\n",
        "            contents = os.listdir(os.path.join(directory,folder))\n",
        "            total_images += len(contents)\n",
        "            if len(contents)<min_count:\n",
        "                min_count = len(contents)\n",
        "            if len(contents)>max_count:\n",
        "                max_count = len(contents)\n",
        "            counts.append((folder,len(contents)))\n",
        "        print ('Classes: ', len(counts), 'Fewest: ', min_count, 'Most: ', max_count)\n",
        "        print ('Number of images: ', total_images)\n",
        "        print ('Counts per class: ', counts)\n",
        "        print ()\n",
        "eda_counts()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5ApneMe-ok-p",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# For the classes in the training set with few exampls, \n",
        "# copy random images in the sub-folder to increase the sample size\n",
        "min_number = 54 #twice the smallest class\n",
        "folders = ([name for name in os.listdir(train_dir)])\n",
        "for folder in folders:\n",
        "    contents = os.listdir(os.path.join(train_dir,folder))\n",
        "    if len(contents) < min_number: \n",
        "        images_to_add = min_number-len(contents)\n",
        "        idx = np.random.choice(len(contents), images_to_add, replace=False)\n",
        "        for x in range(images_to_add):\n",
        "            shutil.copy(train_dir + '/' + str(folder) + '/' + contents[idx[x]], train_dir + '/' + str(folder) + '/cp_' + contents[idx[x]])\n",
        "        print ('sub_folder ', folder, 'copied ' + str(images_to_add) + ' images')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sNTPvB_ZooXt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Label mapping\n",
        "The file cat_to_name.json is provided for the mapping from category label to category name. It's a JSON object which can be read in with the json module. This gives a dictionary mapping the integer encoded categories to the actual names of the flowers."
      ]
    },
    {
      "metadata": {
        "id": "Vv-rR7n6orB5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!wget  -O ./assets/cat_to_name.json \"https://github.com/udacity/pytorch_challenge/blob/master/cat_to_name.json\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rf1-OCMdotwo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import json\n",
        "with open('cat_to_name.json', 'r') as f: cat_to_name = json.load(f) \n",
        "cat_to_name"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CVfKJkX0ov3q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# ImageFolder does not take the folder name as the label; it labels the folders in alphabetical order with numbers \n",
        "# (0,101)\n",
        "names = pd.DataFrame.from_dict(cat_to_name, orient='index', columns=['class'])\n",
        "names=names.sort_index(axis=0)\n",
        "names['labels']=range(102)\n",
        "names.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AKkt1U_PoyIZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Transformations\n",
        "train_data_transforms = transforms.Compose([\n",
        "                        transforms.RandomHorizontalFlip(), # randomly flip and rotate\n",
        "                        transforms.RandomVerticalFlip(),\n",
        "                        transforms.RandomRotation(10),\n",
        "                        transforms.RandomResizedCrop(224),\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Normalize((0.485, 0.456, 0.406), \n",
        "                                             (0.229, 0.224, 0.225))])\n",
        "valid_data_transforms = transforms.Compose([\n",
        "                        transforms.Resize(256),\n",
        "                        transforms.CenterCrop(224),\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Normalize((0.485, 0.456, 0.406), \n",
        "                                             (0.229, 0.224, 0.225))])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-kwDrbTJo0Fm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Number of subprocesses to use for data loading\n",
        "num_workers = 0\n",
        "\n",
        "# Number of samples per batch to load\n",
        "batch_size = 256\n",
        "\n",
        "# Load the datasets with ImageFolder\n",
        "train_data = datasets.ImageFolder(train_dir, transform=train_data_transforms)\n",
        "valid_data = datasets.ImageFolder(valid_dir, transform=valid_data_transforms)\n",
        "\n",
        "valid_batch_size = len(valid_data) #can be bigger since not optimizing grads\n",
        "\n",
        "# Using the image datasets and the transforms, define the dataloaders\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n",
        "                                           num_workers=num_workers, shuffle=True)\n",
        "valid_loader = torch.utils.data.DataLoader(valid_data, batch_size=valid_batch_size,\n",
        "                                           num_workers=num_workers, shuffle=True)\n",
        "# Statistics\n",
        "print('Num training images: ', len(train_data))\n",
        "print('Num valid images: ', len(valid_data))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bw79JeD0o2Vx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class_to_idx = {sorted(train_data.classes)[i]: i for i in range(len(train_data.classes))}\n",
        "{k: class_to_idx[k] for k in list(class_to_idx)[:5]}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "v5v2Ekroo4mn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Reverse dictionary, used in sanity check\n",
        "idx_to_class = {val: key for key, val in class_to_idx.items()}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vyMk6yzIo69Q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Plot an original image from each training folder, and corresponding labels\n",
        "# Choose 60 sub-folders at random\n",
        "sub_folders = np.random.choice(len(class_to_idx), 60, replace=False)+1\n",
        "images=[]\n",
        "for folder in sub_folders:\n",
        "    contents = os.listdir(os.path.join(train_dir,str(folder)))\n",
        "    images.append(Image.open(train_dir + '/' + str(folder) + '/' + contents[0]))\n",
        "fig = plt.figure(figsize=(25, 12))\n",
        "for idx in np.arange(60):\n",
        "    ax = fig.add_subplot(6, 60/6, idx+1, xticks=[], yticks=[])\n",
        "    plt.imshow(images[idx])\n",
        "    title = names['class'][class_to_idx[str(sub_folders[idx])]], names['labels'][class_to_idx[str(sub_folders[idx])]]\n",
        "    ax.set_title(title[0] + ' ' + str(title[1]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0-5xN-2Xo8q7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Visualize sample data from one batch of training data\n",
        "dataiter = iter(train_loader)\n",
        "images, labels = dataiter.next()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RlTbacDlo-uc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Plot the images in the batch, along with the corresponding labels\n",
        "# These images have been transformed\n",
        "fig = plt.figure(figsize=(25, 12))\n",
        "for idx in np.arange(min(60, batch_size)):\n",
        "    ax = fig.add_subplot(6, 60/6, idx+1, xticks=[], yticks=[])\n",
        "    image = images[idx].numpy()\n",
        "    image = image.transpose(1,2,0)\n",
        "    image = image * np.array((0.229, 0.224, 0.225)) + np.array((0.485, 0.456, 0.406))\n",
        "    image = image.clip(0, 1)\n",
        "    plt.imshow(image)\n",
        "    title = names.loc[names['labels'] == labels[idx].numpy(), 'class'].iloc[0]\n",
        "    ax.set_title(title)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Iu5sN5q5pBg4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Building and training the classifier\n",
        "A pretrained model from torchvision.models is used to get the image features. Using those features a new feed-forward classifier is built and trained.\n",
        "\n",
        "Steps:\n",
        "\n",
        "* Load a pre-trained network\n",
        "* Define a new, untrained feed-forward network as a classifier, using ReLU activations and dropout\n",
        "* Train the classifier layers using backpropagation using the pre-trained network to get the features\n",
        "* Track the loss and accuracy on the validation set to determine the best hyperparameters"
      ]
    },
    {
      "metadata": {
        "id": "yJ9gx33CpG47",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# DenseNet201 - the pre-trained network\n",
        "model = models.densenet201(pretrained=True)\n",
        "\n",
        "# Freeze training for all \"features\" layers\n",
        "for param in model.parameters(): param.requires_grad_(False)\n",
        "    \n",
        "# Definition of the classifier    \n",
        "n_inputs = model.classifier.in_features\n",
        "classifier = nn.Sequential(OrderedDict([\n",
        "        ('fc1', nn.Linear(n_inputs, 512)),\n",
        "        ('relu', nn.ReLU()),\n",
        "        ('bn1', nn.BatchNorm1d(512)),\n",
        "        ('drop', nn.Dropout(0.5)),\n",
        "        ('fc2', nn.Linear(512, len(train_data.classes))),\n",
        "        ('softmax', nn.LogSoftmax(dim=1))\n",
        "        ]))\n",
        "model.classifier = classifier\n",
        "\n",
        "criterion = nn.NLLLoss()\n",
        "optimizer = optim.Adam(model.classifier.parameters(), lr=3e-3)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.2)\n",
        "\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f'{total_params:,} total parameters.')\n",
        "total_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f'{total_trainable_params:,} training parameters.')\n",
        "\n",
        "train_on_gpu = torch.cuda.is_available()\n",
        "if train_on_gpu: model = model.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RLRcyiPPpL8A",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# TRAINING - typically gets to 97% around 15 epochs\n",
        "n_epochs, valid_loss_min, trn_loss_hist, val_loss_hist, best_acc  = 20, 100, [], [], 0.0\n",
        "since = time.time()\n",
        "for epoch in range(1, n_epochs+1):\n",
        "    train_loss, valid_loss, correct = 0.0, 0.0, 0.0 \n",
        "    scheduler.step()\n",
        "    model.train()\n",
        "    for data, target in train_loader:\n",
        "        if train_on_gpu: data, target = data.cuda(), target.cuda()\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data) \n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()*data.size(0)\n",
        "    model.eval()\n",
        "    for data, target in valid_loader:\n",
        "        if train_on_gpu: data, target = data.cuda(), target.cuda()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        valid_loss += loss.item()*data.size(0)\n",
        "        # Get the index of the max log-probability\n",
        "        _, pred = output.max(1, keepdim=True) \n",
        "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "    train_loss, valid_loss  = train_loss/len(train_loader.dataset), valid_loss/len(valid_loader.dataset)\n",
        "    trn_loss_hist.append(train_loss)\n",
        "    val_loss_hist.append(valid_loss)\n",
        "    epoch_acc = correct/len(valid_data)\n",
        "    if epoch_acc > best_acc:\n",
        "        best_acc = epoch_acc\n",
        "        torch.save(model, 'flowers'+str(epoch)+'.pth')\n",
        "    print(f'Epoch: {epoch} Training Loss: {train_loss:.4f} Validation Loss: {valid_loss:.4f} Accuracy: {epoch_acc:.4f}')\n",
        "time_elapsed = time.time() - since\n",
        "print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-EotJMnkpNzm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def plot_losses():\n",
        "    ''' Plot training and validation losses\n",
        "    '''\n",
        "    plt.title(\"Training and Validation Losses\")\n",
        "    plt.xlabel(\"Training Epochs\")\n",
        "    plt.ylabel(\"Losses\")\n",
        "    plt.plot(range(1,len(trn_loss_hist)+1),trn_loss_hist,label=\"Training\")\n",
        "    plt.plot(range(1,len(val_loss_hist)+1),val_loss_hist,label=\"Validation\")\n",
        "    plt.ylim((0,min([1, max(trn_loss_hist)])))\n",
        "    plt.xticks(np.arange(1, n_epochs+1, 1.0))\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JU60CqyEpQfR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plot_losses()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8o3hCqRDpSXf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = torch.load('flowers16.pth')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-4rlB1LbpUDu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Unfreeze model and train further with a smaller batch size and a small learning rate\n",
        "num_workers = 0\n",
        "batch_size = 32 \n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n",
        "                                           num_workers=num_workers, shuffle=True)\n",
        "valid_loader = torch.utils.data.DataLoader(valid_data, batch_size=batch_size,\n",
        "                                           num_workers=num_workers, shuffle=True)\n",
        "for param in model.parameters(): param.requires_grad_(True)\n",
        "criterion = nn.NLLLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=3e-5)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.2)\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f'{total_params:,} total parameters.')\n",
        "total_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f'{total_trainable_params:,} training parameters.')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "X5o-Qy4HpWvz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Further training - typically gets to 99% in around 10 epochs\n",
        "train_on_gpu = torch.cuda.is_available()\n",
        "n_epochs, valid_loss_min, trn_loss_hist, val_loss_hist, best_acc  = 20, 100, [], [], 0.\n",
        "since = time.time()\n",
        "for epoch in range(1, n_epochs+1):\n",
        "    train_loss, valid_loss, correct = 0.0, 0.0, 0.0\n",
        "    scheduler.step()\n",
        "    model.train()\n",
        "    for data, target in train_loader:\n",
        "        if train_on_gpu: data, target = data.cuda(), target.cuda()\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data) \n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()*data.size(0)\n",
        "    model.eval()\n",
        "    for data, target in valid_loader:\n",
        "        if train_on_gpu: data, target = data.cuda(), target.cuda()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        valid_loss += loss.item()*data.size(0)\n",
        "        _, pred = output.max(1, keepdim=True) \n",
        "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "    train_loss, valid_loss  = train_loss/len(train_loader.dataset), valid_loss/len(valid_loader.dataset)\n",
        "    trn_loss_hist.append(train_loss)\n",
        "    val_loss_hist.append(valid_loss)\n",
        "    epoch_acc = correct/len(valid_data)\n",
        "    if epoch_acc > best_acc:\n",
        "        best_acc = epoch_acc\n",
        "        torch.save(model, 'flowers_unfr_'+str(epoch)+'.pth')\n",
        "    print(f'Epoch: {epoch} Training Loss: {train_loss:.4f} Validation Loss: {valid_loss:.4f} Accuracy: {epoch_acc:.4f}')\n",
        "time_elapsed = time.time() - since\n",
        "print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dhbaSIffpZHJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plot_losses()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "us_OhAB3pbZ_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Test Accuracy on the best model\n",
        "model = torch.load('flowers_unfr_18.pth') \n",
        "train_on_gpu = torch.cuda.is_available()\n",
        "test_data = datasets.ImageFolder(test_dir, transform=valid_data_transforms)\n",
        "test_batch_size = 32\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=test_batch_size)\n",
        "correct = 0\n",
        "class_errors = []\n",
        "for data, target in test_loader:\n",
        "    if train_on_gpu: data, target = data.cuda(), target.cuda()\n",
        "    output = model(data)\n",
        "    _, pred = output.max(1, keepdim=True)\n",
        "    correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "    indices = [i for i, x in enumerate(pred.eq(target.view_as(pred))) if x == 0]\n",
        "    class_errors.append(target[indices].tolist())\n",
        "acc = correct/len(test_data)\n",
        "print(f'Accuracy: {acc:.4f} Correctly identified: {correct} Incorrectly identified: {len(test_data)-correct}')\n",
        "print (f'Classes with incorrectly identified flowers: {class_errors}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "T8lYTLoRpd1g",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Final submission - could combine train and valid data, \n",
        "# then run same training as used to get best performing (flowers_unfr_18.pth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1PQH3PuIphgv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Save and loading a checkpoint\n",
        "Once the network is trained, the model can be saved so that it can be loaded later for making predictions. If the model is to be used for further training, the number of epochs should be saved as well as the optimizer state, optimizer.state_dict."
      ]
    },
    {
      "metadata": {
        "id": "jzGHuydoplVa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# For the Challenge TEST, save a checkpoint\n",
        "# Load a saved model that performed well in training\n",
        "model = torch.load('flowers_unfr_18.pth') \n",
        "model.cpu()\n",
        "states = {'class_to_idx': train_data.class_to_idx,\n",
        "          'state_dict': model.state_dict()}\n",
        "torch.save(states, 'flowers_state_dict.pth')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xTNPKPcwpnHJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Loading the checkpoint\n",
        "A function is needed that can load a checkpoint and rebuild the model."
      ]
    },
    {
      "metadata": {
        "id": "hORAbgcbpo40",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# DenseNet201\n",
        "import torch\n",
        "from torch import nn\n",
        "from torchvision import models\n",
        "from collections import OrderedDict\n",
        "\n",
        "def load_checkpoint(checkpoint_path):\n",
        "    \n",
        "    checkpoint = torch.load(checkpoint_path, map_location=lambda storage, loc: storage)\n",
        "    model = models.densenet201(pretrained=False)\n",
        "    for param in model.parameters(): param.requires_grad = False\n",
        "    n_inputs = model.classifier.in_features\n",
        "    classifier = nn.Sequential(OrderedDict([\n",
        "        ('fc1', nn.Linear(n_inputs, 512)),\n",
        "        ('relu', nn.ReLU()),\n",
        "        ('bn1', nn.BatchNorm1d(512)),\n",
        "        ('drop', nn.Dropout(0.5)),\n",
        "        ('fc2', nn.Linear(512, len(train_data.classes))),\n",
        "        ('softmax', nn.LogSoftmax(dim=1))\n",
        "        ]))\n",
        "    model.classifier = classifier\n",
        "    model.load_state_dict(checkpoint['state_dict'], strict=False)\n",
        "    model.class_to_idx = checkpoint['class_to_idx']\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "model = load_checkpoint('flowers_state_dict.pth')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NwQbCd8ipulr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Inference for classification\n",
        "A function is written to use a trained network for inference. An image is passed into the network and the class of the flower in the image is predicted.\n",
        "\n",
        "The predict function takes an image and a model, then returns the top $K$ most likely classes along with the probabilities.\n",
        "\n",
        "First the input image has to be processed such that it can be used by the model.\n",
        "\n",
        "# Image Preprocessing\n",
        "PIL is used to load the image (documentation). The function preprocesses the image in the same manner used for training so it can be used as input for the model.\n",
        "\n",
        "First, the images are resized where the shortest side is 256 pixels, keeping the aspect ratio. This can be done with the thumbnail or resize methods.\n",
        "\n",
        "Then a crop of the center 224x224 portion of the image is taken.\n",
        "\n",
        "Colour channels of images are typically encoded as integers 0-255, but the model expected floats 0-1, so values are converted. This is easier with a Numpy array, so np_image = np.array(pil_image).\n",
        "\n",
        "The network expects the images to be normalized in a specific way. [0.485, 0.456, 0.406] for the means and [0.229, 0.224, 0.225] for the standard deviations . Tthe means have to be subtracted from each color channel, then the channels are divided by the standard deviation.\n",
        "\n",
        "And finally, PyTorch expects the color channel to be the first dimension but it's the third dimension in the PIL image and Numpy array. Ddimensions are reordered using ndarray.transpose. The colour channel needs to be first and the order retained of the other two dimensions."
      ]
    },
    {
      "metadata": {
        "id": "PsGkp71_p0Gv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def process_image(image_path):\n",
        "    ''' Scales, crops, and normalizes a PIL image for a PyTorch model,\n",
        "        returns an Numpy array\n",
        "    '''\n",
        "    img = Image.open(image_path)\n",
        "    # Resize\n",
        "    if img.size[0] > img.size[1]:\n",
        "        img.thumbnail((10000, 256), Image.ANTIALIAS)\n",
        "    else:\n",
        "        img.thumbnail((256, 10000), Image.ANTIALIAS)\n",
        "    # Centre crop\n",
        "    left_margin = (img.width-224)/2\n",
        "    bottom_margin = (img.height-224)/2\n",
        "    right_margin = left_margin + 224\n",
        "    top_margin = bottom_margin + 224\n",
        "    img = img.crop((left_margin, bottom_margin, right_margin, top_margin))\n",
        "    # Normalize\n",
        "    img = np.array(img)/255\n",
        "    img = (img - np.array([0.485, 0.456, 0.406]))/np.array([0.229, 0.224, 0.225])\n",
        "    # Change channel order\n",
        "    img = img.transpose((2, 0, 1))\n",
        "    return img\n",
        "\n",
        "image = process_image(train_dir + '/54/image_05459.jpg')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vxJvcUsDp19T",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "As a check, if the process_image function works, running the output through this function displays the original image (except for the cropped out portions)."
      ]
    },
    {
      "metadata": {
        "id": "08JbJfZsp4il",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def imshow(image, ax=None, title=None):\n",
        "    ''' Takes a processed image and undoes the processing to display the image\n",
        "    '''\n",
        "    if ax is None: fig, ax = plt.subplots()\n",
        "    # PyTorch tensors assume the colour channel is the first dimension\n",
        "    # but matplotlib assumes is the third dimension\n",
        "    image = image.transpose((1, 2, 0))\n",
        "    # Undo normalizing\n",
        "    mean = [0.485, 0.456, 0.406]\n",
        "    std = [0.229, 0.224, 0.225]\n",
        "    image = std * image + mean\n",
        "    ax.imshow(image)\n",
        "    return ax\n",
        "\n",
        "imshow(image)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JJD_k7NDp7AD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Class Prediction\n",
        "Once the images are in the correct format, predictions can be made with the model. A common practice is to predict the top 5 or so (usually called top-$K$) most probable classes. The class probabilities are calculated then the $K$ largest values are found.\n",
        "\n",
        "To get the top $K$ largest values in a tensor x.topk(k) is used. This method returns both the highest k probabilities and the indices of those probabilities corresponding to the classes. class_to_idx converts from these indices to the actual class labels. The reverse dictionary gives a mapping from index to class."
      ]
    },
    {
      "metadata": {
        "id": "yKmprflYp-5k",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def predict(image_path, model, top_num=5):\n",
        "    ''' Predict the class (or classes) of an image using a trained deep learning model.\n",
        "    '''\n",
        "    image = process_image(image_path)\n",
        "    image_tensor = torch.from_numpy(image).type(torch.cuda.FloatTensor) #turn into a tensor\n",
        "    model_input = image_tensor.unsqueeze(0) # add batch of size 1 to image\n",
        "    output = model(model_input) # run image through model\n",
        "    preds, classes = output.topk(top_num)\n",
        "    preds = torch.exp(preds.detach()[0])\n",
        "    classes = classes[0]\n",
        "    cats = [idx_to_class[clas.item()] for clas in classes]\n",
        "    flowers = [cat_to_name[cat] for cat in cats]\n",
        "    return preds, flowers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ekYsccwOqBZa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Sanity Checking\n",
        "Plot the probabilities for the top 5 classes as a bar graph, along with the input image."
      ]
    },
    {
      "metadata": {
        "id": "QSqc-sfIqGnR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def plot_solution(image_path, model):\n",
        "    ''' Plot side by side the top predictions for a flower\n",
        "        and the image of the file\n",
        "    '''\n",
        "    preds, flowers = predict(image_path, model)\n",
        "    text_colour = 'r'\n",
        "    plt.figure(figsize = (12,6))\n",
        "    ax = plt.subplot(1,2,2)\n",
        "    title = cat_to_name[image_path.split('/')[-2]] + ' / predicted: ' + flowers[0]\n",
        "    if cat_to_name[image_path.split('/')[-2]] == flowers[0]: text_colour = 'g'\n",
        "    img = process_image(image_path)\n",
        "    plt.title(title, color=text_colour)\n",
        "    imshow(img, ax, title);\n",
        "    plt.subplot(1,2,1)\n",
        "    plt.barh(flowers, preds.detach().cpu().numpy(), align='center', color=text_colour)\n",
        "    plt.xlim((0,1))\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4q5Zf3ItqIkU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Checking model predictions for individual folders in the test set where flowers were misclassified\n",
        "# e.g. classes 15, 17, 21, 35, 37, 98\n",
        "folder = str(idx_to_class[98])\n",
        "images = os.listdir(os.path.join(test_dir,folder))\n",
        "for image in images:\n",
        "    image_path = test_dir + '/' + folder + '/' + image\n",
        "    plot_solution(image_path, model)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}